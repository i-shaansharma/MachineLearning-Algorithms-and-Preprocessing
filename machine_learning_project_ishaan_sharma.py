# -*- coding: utf-8 -*-
"""MACHINE LEARNING PROJECT ISHAAN SHARMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUOLNzApiNaTEXqCwyJimbIGSeXPP-Wl
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression, Ridge, Lasso, LinearRegression, Perceptron
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import silhouette_score

iris = pd.read_csv('Iris.csv')  # Replace with your dataset path
salary = pd.read_csv('Salary_dataset.csv')  # Replace with your dataset path
clustering = pd.read_csv('data.csv')  # Replace with your dataset path

print(iris.head())
print(salary.head())
print(clustering.head())

### PART 1: DATA LOADING AND PREPROCESSING ###

# 3. Missing Value Handling
print("Missing Values in Iris Dataset:\n", iris.isnull().sum())
print("Missing Values in Salary Dataset:\n", salary.isnull().sum())
print("Missing Values in Clustering Dataset:\n", clustering.isnull().sum())

# Deletion (dropping rows with missing values)
iris.dropna(inplace=True)
salary.fillna(salary.mean(), inplace=True)  # Impute missing values with mean for numerical columns

# 4. Encoding Categorical Features (for classification task)
label_encoder = LabelEncoder()
iris['Species'] = label_encoder.fit_transform(iris['Species'])

# 5. Train-Test Split (80-20 split, stratified for classification)
X_iris = iris.drop('Species', axis=1)
y_iris = iris['Species']
X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris)

# Independent variable (YearsExperience)
X_salary = salary[['YearsExperience']]  # Independent variable

# Dependent variable (Salary)
y_salary = salary['Salary']  # Dependent variable

# Train-test split
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_salary, y_salary, test_size=0.2, random_state=42)

# Feature Scaling: Standardization and Normalization
scaler = StandardScaler()
X_train_cls_std = scaler.fit_transform(X_train_cls)
X_test_cls_std = scaler.transform(X_test_cls)

min_max_scaler = MinMaxScaler()
X_train_cls_mm = min_max_scaler.fit_transform(X_train_cls)
X_test_cls_mm = min_max_scaler.transform(X_test_cls)

### PART 2: FEATURE ENGINEERING ###

# 6. Feature Selection using RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_cls_std, y_train_cls)
selector = SelectFromModel(rf, prefit=True)
X_train_fs = selector.transform(X_train_cls_std)
X_test_fs = selector.transform(X_test_cls_std)

# 7. Dimensionality Reduction (PCA)
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_fs)
X_test_pca = pca.transform(X_test_fs)

# 8. Class Balance Check
sns.countplot(x='Species', data=iris)
plt.title('Class Distribution in Iris Dataset')
plt.show()

# SMOTE (for balancing class distribution)
smote = SMOTE(random_state=42)
X_train_cls_smote, y_train_cls_smote = smote.fit_resample(X_train_cls, y_train_cls)

### PART 3: MODEL BUILDING ###

# 9. Classification Models
models_cls = {
    'Perceptron': Perceptron(),
    'Logistic Regression': LogisticRegression(),
    'SVM (Linear)': SVC(kernel='linear'),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Gaussian Naive Bayes': GaussianNB()
}

# Train and Evaluate Classification Models
for name, model in models_cls.items():
    model.fit(X_train_pca, y_train_cls)
    y_pred = model.predict(X_test_pca)
    print(f"{name} Accuracy: {accuracy_score(y_test_cls, y_pred):.2f}")
    print(classification_report(y_test_cls, y_pred))

# 10. Regression Models
models_reg = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Decision Tree Regressor': DecisionTreeRegressor()
}

# Train and Evaluate Regression Models
for name, model in models_reg.items():
    model.fit(X_train_reg, y_train_reg)
    y_pred_reg = model.predict(X_test_reg)
    mse = mean_squared_error(y_test_reg, y_pred_reg)
    r2 = r2_score(y_test_reg, y_pred_reg)
    print(f"{name} MSE: {mse:.2f}, R2: {r2:.2f}")

# 11. Clustering Models
clustering_models = {
    'K-Means': KMeans(n_clusters=3, random_state=42),
    'Agglomerative Clustering': AgglomerativeClustering(n_clusters=3),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5)
}

# Train and Evaluate Clustering Models
for name, model in clustering_models.items():
    model.fit(clustering[['x', 'y']])  # assuming 'x' and 'y' are the feature columns for clustering

    # Check if the model has labels_ attribute (common for models like DBSCAN, Agglomerative)
    if hasattr(model, 'labels_'):
        labels = model.labels_
    else:
        labels = model.predict(clustering[['x', 'y']])

    # Check if the number of unique labels is greater than 1
    if len(set(labels)) > 1:
        silhouette = silhouette_score(clustering[['x', 'y']], labels)
        print(f"{name} Silhouette Score: {silhouette:.2f}")
    else:
        print(f"{name} Silhouette Score: Cannot be computed (only 1 cluster)")

# 12. Ensemble Models
ensemble_models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Bagging': BaggingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Voting Classifier': VotingClassifier(estimators=[('lr', LogisticRegression()), ('rf', RandomForestClassifier())])
}

# Train and Evaluate Ensemble Models
for name, model in ensemble_models.items():
    model.fit(X_train_pca, y_train_cls)
    y_pred = model.predict(X_test_pca)
    print(f"{name} Accuracy: {accuracy_score(y_test_cls, y_pred):.2f}")

### PART 4: MODEL VALIDATION & EVALUATION ###

# 13. Validation using Cross-Validation
for name, model in models_cls.items():
    scores = cross_val_score(model, X_train_pca, y_train_cls, cv=5)
    print(f"{name} Cross-validation Accuracy: {scores.mean():.2f}")

# 14. Hyperparameter Tuning using GridSearchCV
param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid_search.fit(X_train_pca, y_train_cls)
print(f"Best Parameters for KNN: {grid_search.best_params_}")

# 15. Outlier Detection using Z-Score Method
from scipy import stats
z_scores = np.abs(stats.zscore(X_train_cls))
print(f"Z-Scores:\n{z_scores}")

# 16. Evaluation Metrics (Accuracy, Precision, Recall, etc.)
y_pred = models_cls['Logistic Regression'].predict(X_test_pca)
print("Confusion Matrix:\n", confusion_matrix(y_test_cls, y_pred))
print("Classification Report:\n", classification_report(y_test_cls, y_pred))

### PART 5: VISUALIZATION ###

# 17. EDA Plots: Histograms and Scatter Plots
sns.histplot(iris['SepalLengthCm'], kde=True)
plt.title('Distribution of Sepal Length in Iris')
plt.show()

sns.scatterplot(x=iris['SepalLengthCm'], y=iris['SepalWidthCm'], hue=iris['Species'])
plt.title('Sepal Length vs Sepal Width')
plt.show()

# 18. Dimensionality Reduction Visualization
sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train_cls)
plt.title('PCA for Iris Dataset')
plt.show()

### PART 6: PIPELINE CREATION ###

# Create Pipeline for Classification
pipeline_cls = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])
pipeline_cls.fit(X_train_cls, y_train_cls)
print(f"Pipeline Accuracy: {pipeline_cls.score(X_test_cls, y_test_cls):.2f}")

# Store accuracy for classification models
accuracy_scores_cls = {}
for name, model in models_cls.items():
    model.fit(X_train_pca, y_train_cls)
    y_pred = model.predict(X_test_pca)
    accuracy_scores_cls[name] = accuracy_score(y_test_cls, y_pred)

# Store R2 for regression models
r2_scores_reg = {}
for name, model in models_reg.items():
    model.fit(X_train_reg, y_train_reg)
    y_pred_reg = model.predict(X_test_reg)
    r2_scores_reg[name] = r2_score(y_test_reg, y_pred_reg)

from sklearn.metrics import silhouette_score

# Store silhouette score for clustering models
silhouette_scores_clust = {}
for name, model in clustering_models.items():
    model.fit(clustering[['x', 'y']])  # assuming 'x' and 'y' are the feature columns for clustering
    if hasattr(model, 'labels_'):
        labels = model.labels_
    else:
        labels = model.predict(clustering[['x', 'y']])

    # Check if there are more than 1 cluster (to calculate silhouette score)
    if len(np.unique(labels)) > 1:
        silhouette = silhouette_score(clustering[['x', 'y']], labels)
        silhouette_scores_clust[name] = silhouette
    else:
        silhouette_scores_clust[name] = None  # If only 1 cluster, store None

# Print silhouette scores (some might be None)
print("Silhouette Scores for Clustering Models:")
for model_name, score in silhouette_scores_clust.items():
    print(f"{model_name}: {score}")

# 2. Plotting the Model Performance Comparison

# Prepare data for plotting
metrics = {
    'Classification Accuracy': accuracy_scores_cls,
    'Regression R2': r2_scores_reg,
    'Clustering Silhouette': silhouette_scores_clust
}

# Create a DataFrame for the comparison
metrics_df = pd.DataFrame(metrics)

# Plotting the performance comparison
plt.figure(figsize=(12, 6))

# Bar Plot for Comparison
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title('Model Performance Comparison')
plt.xlabel('Models')
plt.ylabel('Scores')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

# Show the plot
plt.show()

# 3. Conclusion
print("Conclusion:")
print("1. The classification models were evaluated using accuracy.")
print("2. The regression models were evaluated using RÂ².")
print("3. The clustering models were evaluated using silhouette score.")
print("4. Based on the bar plot, the best performing models can be selected for further tuning.")